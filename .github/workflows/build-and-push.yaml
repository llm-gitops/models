name: build-and-push

on:
  push:
    branches:
      - main  # or whichever branch you'd like to target

permissions:
  packages: write # needed for ghcr.io access

jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      # OLD oci://ghcr.io/llm-gitops/models/vicuna-13b-16k:v1.5-ggmlv3
      # <model-name>-<param size>-<context size>-<quantization>:<version>-<format>-<release>
      # llama-2-7b-chat-4k-Q5_K_M:v1-gguf-r0
      matrix:
        config:
          - repo: "TheBloke/Llama-2-7b-Chat-GGUF"
            file: "llama-2-7b-chat.Q5_K_M.gguf"
            image: "llama-2-7b-chat-4k:v1.0.0-q5km-gguf"

          - repo: "TheBloke/Llama-2-7b-GGUF"
            file: "llama-2-7b.Q2_K.gguf"
            image: "llama-2-7b-4k:v1.0.0-q2k-gguf"

          - repo: "TheBloke/vicuna-13B-v1.5-16K-GGUF"
            file: "vicuna-13b-v1.5-16k.Q5_K_M.gguf"
            image: "vicuna-13b-16k:v1.5.0-q5km-gguf"

          - repo: "TheBloke/Llama-2-7B-32K-Instruct-GGUF"
            file: "llama-2-7b-32k-instruct.Q5_K_M.gguf"
            image: "llama-2-7b-instruct-32k:v1.0.0-q5km-gguf"

          - repo: "TheBloke/sqlcoder-GGUF"
            file: "sqlcoder.Q4_K_M.gguf"
            image: "sqlcoder:v1.0.0-q4km-gguf"

          - repo: "skeskinen/ggml"
            file: "all-MiniLM-L12-v2/ggml-model-q4_1.bin"
            image: "all-minilm-l12-v2:v1.0.0-q41-ggml"

          - repo: "skeskinen/ggml"
            file: "all-MiniLM-L6-v2/ggml-model-q4_1.bin"
            image: "all-minilm-l6-v2:v1.0.0-q41-ggml"

    steps:
    # Checkout your repo
    - name: Checkout code
      uses: actions/checkout@v2

    # Install Python 3.10
    - name: Set up Python 3.10
      uses: actions/setup-python@v2
      with:
        python-version: "3.10"

    # Install Flux v2 CLI
    - name: Setup Flux CLI
      uses: fluxcd/flux2/action@main

    # Install huggingface_hub package
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install huggingface_hub

    # Run your script
    # - name: Download model
    #   run: |
    #     python download.py

    #     cd model
    #     target=$(readlink vicuna-13b-v1.5-16k.ggmlv3.q4_1.bin)
    #     cp --remove-destination $target vicuna-13b-v1.5-16k.ggmlv3.q4_1.bin
    #     rm -rf $target

    - name: Download model
      env:
        MODEL_REPO: ${{ matrix.config.repo }}
        MODEL_FILE: ${{ matrix.config.file }}
      run: |
        python download.py

        cd model
        rm .keep # delete the place holder file
        
        MODEL_DIR=$PWD
        pushd .
        tree .
        TARGET=$(readlink ${{ matrix.config.file }})
        cd $(dirname ${{ matrix.config.file }})
        cp --remove-destination $TARGET $(basename ${{ matrix.config.file }})
        popd
        rm -rf $TARGET
        
        # flatten structure
        find . -mindepth 2 -type f | while read -r file; do
            # Move each file to the /models directory
            mv "$file" "${MODEL_DIR}/$(basename "$file")"
        done
        # once all files have been moved, find and remove any empty directories
        find . -type d -empty -delete

    - name: Push model
      run: |
        cd model
        tree .
        flux push artifact \
          --creds chanwit:${{ secrets.GHCR_TOKEN }} \
          oci://ghcr.io/llm-gitops/models/${{ matrix.config.image }} \
          --timeout 120m0s \
          --path="./" \
          --source="$(git config --get remote.origin.url)" \
          --revision="$(git branch --show-current)@sha1:$(git rev-parse HEAD)"
