apiVersion: source.toolkit.fluxcd.io/v1beta2
kind: OCIRepository
metadata:
  name: llama-2-7b-instruct-32k
  namespace: flux-system
  labels:
    artifact-kind: language-model
  annotations:
    ai.contrib.fluxcd.io/original-model-repo: https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct
    ai.contrib.fluxcd.io/model-repo: TheBloke/Llama-2-7B-32K-Instruct-GGUF
    ai.contrib.fluxcd.io/model-file: llama-2-7b-32k-instruct.Q5_K_M.gguf
    ai.contrib.fluxcd.io/model-version: 1.0.0
    ai.contrib.fluxcd.io/format: GGUFv2
    ai.contrib.fluxcd.io/family: llama # backend
    ai.contrib.fluxcd.io/context-size: 32768
    ai.contrib.fluxcd.io/quantization: Q5_K_M
    ai.contrib.fluxcd.io/size-on-disk: 4.78G
    ai.contrib.fluxcd.io/memory-required: 7.28G
    ai.contrib.fluxcd.io/layers: 32 # which can be offloaded to GPU
    ai.contrib.fluxcd.io/prompt-template: |-
      [INST]
      {prompt}
      [\INST]
spec:
  interval: 10m
  url: oci://ghcr.io/llm-gitops/models/llama-2-7b-instruct-32k
  ref:
    tag: v1.0.0-q5km-gguf
  layerSelector:
    mediaType: "application/vnd.cncf.flux.content.v1.tar+gzip"
    operation: copy
